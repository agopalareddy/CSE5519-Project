% CVPR 2022 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
% \usepackage{cvpr}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\include{metadata.tex}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Red-Blue Visual Auto Defender}

\author{
Stuart Aldrich \hspace{1in} Mohammad Rouie Miab \hspace{1in} Aadarsha Gopala Reddy
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
Jailbreaks are an increasing problem in the LLM space, and vision language models provide an additional attack surface.
Much of the existing work focuses on utilizing machine learning models to prevent attacks \cite{10657949}.
This presents potentially an additional attack surface where the defense model itself could be attacked \cite{299563}.
And most attacks are focused on traditional classifier-style attacks with image perturbation \cite{10.5555/3698900.3699069}.
This creates an opening for an entire new class of attacks: image-based prompt injections.
We look to provide a tool to automatically defend against jailbreaks against a vision language model in the image space.
In this project, we will create a tool to automatically generate attacks against VLMs and automatically create defenses against attacks.
We will test against a simulation of a real agent in a real domain, such as an email inbox management AI agent.
Our system creates an image generation prompt containing an attack, generates an image with that attack embedded, tests the attack against a target agent, utilizes a VLM to describe the attack image, and generates a Python script to detect the attack based on the description.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Project Overview}

\subsection{Problem}
Vision Language Models (VLMs) are systems that ingest text and images simultaneously and generate a resulting text based on the given input.
These VLMs are increasingly utilized as core components in AI agents, for example, email agents to automatically summarize and respond to emails in a user's inbox.
In traditional LLMs, jailbreaking and prompt injection attacks are commonly known to cause LLMs to behave maliciously in the text domain.
However, VLMs provide an additional attack surface through the image input component.
Protecting VLMs from jailbreaks is an underexplored area and is often done through machine learning models.
While a machine learning model can provide effective protection, it often has poor explainability and may not have deterministic results\cite{qi_visual_2024}\cite{ying_jailbreak_2025}. 
It may additionally be weak to attacks intended for targeting image classifiers\cite{jeong_playing_2025}\cite{wang_steering_2025}.
\subsection{Method}
We seek to provide a method that improves on all three: improved explainability of classification, deterministic results, and not being vulnerable to attacks specifically targeting machine learning models.
Our approach is to create a red-blue teaming approach to automatically create attacks on VLMs, then generate defensive tools to detect the generated attacks.
The specific planned method is to: 

\begin{enumerate}
\item Utilize an attacking (red team) VLM to generate an attacking prompt. 
\item Generate an image containing the attack prompt. 
\item Describe the contents of the attacking image through a VLM. 
\item Using a defending (blue team) VLM, generate a Python script that can parse the image description to detect the attack.
\end{enumerate}

This method will be executed in a refinement loop on both the red and blue team components.
The red team component will refine the attack until it succeeds on a target VLM agent system by detecting if a goal has been achieved.
The blue team component will refine the defense until it detects the attack and allows a set of non-attack images.
Once both components are refined, it will repeat the process N times until the defensive script can defend against N attacks derived from a given starting attack.
The process is then repeated through other base attacks, including attacks attempting to target the defensive system itself.

Once the full system refinement process is completed, the target VLM agent will have a set of defensive scripts able to detect a spectrum of attacks from a single image description.
These scripts will be computationally efficient, deterministic, and explainable.

\subsection{Minimum Expected Goals}
We at a minimum expect to have a system able to generate a single attack that has a single detection script.

\subsection{Maximum Expected Goals}
We expect at most to have a system to create a large variety of attacks and defenses, including the ability to defend against attacks targeting the defensive system itself for weaknesses.

\section{Team Member Roles/Tasks}
\label{sec:roles}

We intentionally share building attacker and defender roles, as based on Stuart's experience with a related project, multiple students working in parallel allows for better exploration and helps avoid getting stuck. Once each student has working versions, they will be combined, utilizing the best elements of each.

\subsection{Stuart Aldrich}

\begin{enumerate}

\item \textbf{Prototype Attacks} Create working case study attack examples where the attacker successfully creates a malicious image that performs prompt injection to the target agent.
\item \textbf{Prototype Defenses} Create case study examples where the defender blocks some attacks.
\item \textbf{Literature Review} Look towards existing literature to attempt to enhance the core ideas. 

\end{enumerate}

\subsection{Mohammad Rouie Miab}

\begin{enumerate}

\item \textbf{Safe-image dataset selection and preparation:} Identify and curate a diverse and robust set of benign images for testing the defender. This involves additionally organizing images into train, test, and validation splits, as well as any preprocessing scripts.
\item \textbf{Attack image generator:} Implement a pipeline that deterministically creates images with embedded attack prompts as text and visuals. Resultant images are then readily usable for the VLM.
\item \textbf{Analysis of the results and performance:} Analyze and summarize performance measurements of attacks and defenses across experiments. Analysis will also include commentary on failure points and recommendations for future improvements.

\end{enumerate}

\subsection{Aadarsha Gopala Reddy}
\begin{enumerate}
    \item \textbf{Develop the Attack Success-Condition Module:} Design and implement the goal achievement detector for the red team. This involves defining success metrics for an attack (e.g., the target VLM producing a harmful response) and writing scripts to automatically parse the VLM's output to determine if an attack was successful.
    
    \item \textbf{Implement the Defense Validation Framework:} Build the system that executes the VLM-generated Python detection scripts. This framework will test the scripts against both malicious and benign images to measure their precision, recall, and overall effectiveness.
    
\end{enumerate}

\section{Resources}

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{Bliss_(Windows_XP).png}
    \caption{Example of safe image made into an unsafe image with prompt injection}
    \label{fig:placeholder}
\end{figure}

\begin{enumerate}
    \item Computer (have already)
    \item Computer to run local models (have already)
    \item API access (have already)
    \item Safe image dataset (need to pick one of the many existing ones)
\end{enumerate}

\section{Reservations}

The primary potential limitation is that, due to safeguards with existing VLMs and image generators, jailbreaks may be generated.
If this fails, we may need to switch to utilizing existing VLM attack datasets or methods.
Additionally, the other limitation is that the VLM may not reliably generate an image description that allows for attack detection. 
For example, some adversarial images potentially contain multiple meanings, and the VLM only outputs one based on context.
If we are unable to defend against adversarial images, then we need to acknowledge the limitations of our system in the final report.
We will additionally attempt to look at other methods, such as alternative image processing techniques with deterministic results, we can potentially use to detect attacks.  

\section{Threat Model}
The threat model outlines and explicates the relevant assets, adversary goals, assumptions, and potential threats for visual jailbreaks against VLMs.

\subsection{Assets protected}
Fundamentally, the integrity and security of the user's data and privacy should remain intact, untouched, and unleaked to any outside adversaries. In addition, intended safe behavior and goals of the user agent should remain unaffected. This is critical to maintain a seamless defense and user experience.

\subsection{Adversary goals}
The primary goal of an adversary is to target and successfully "jailbreak" the VLM agent to produce a harmful or privileged response when presented with an image. Secondarily, the adversary may intend to evade and disable any detection behavior or even potentially poison training/testing/validation datasets to falsely conceal the effectiveness of attacks.

\subsection{Adversary capabilities}
At a basic level, the adversary is capable of specialized crafting images that overlay readable instructions as text or visual cue elements that get submitted to the agent. A more advanced attacker can use dynamic image generation or visual design techniques such as steganography to hide malicious instructions in normal-appearing images. At the extreme, an adversary may employ adaptive tactics such as the use of knowledge of the detection algorithm or iterative querying against the VLM to refine its attacks against the model's behavior.

\subsection{Attack surface}
An adversary can employ such an attack through a multitude of different aspects of the VLM agent's inputs. Most obviously, the image itself can be manipulated to include embedded text or visual cueing either through visible text elements, specialized/stylized fonts, or contrasting colors that may resemble text. In addition, implicit injections of colors, textures, and patterns may cause the VLM to interpret malicious semantic content as well. More internally, biases and incompleteness in a VLM's capabilities, in its image-to-text process, can easily misrepresent or even omit entirely any attack content.

\subsection{Trust model}
In this threat model, several assumptions are made and accepted regarding trusted components in the pipeline. It is assumed that the image generation and VLM processes behave consistently and predictably during experimentation and that detection scripts are deterministic and run in a trusted, secure environment. Also, it is assumed that datasets of benign images are truly safe and not adversarially controlled (not poisoned) unless otherwise specifically tested.

\section{Relationship to Background}

Stuart has experience in a very similar (currently unpublished) project operating solely in the text domain.
This is an extension of that project's ideas into the image domain.

Aadarsha brings experience in multimodal machine learning from his previous projects relating to computational neuroscience, where he developed a pipeline to analyze both MRI scans and textual clinical data for diagnostic predictions. For this project, he can apply his skills to the development of the defender's image description parser and the attacker's goal achievement detector.

Mohammad has extensive prior experience working with machine learning models like LLMs, LSTMs, and CNN-based architectures. In addition, he has a significant amount of background in cyber-physical computer vision projects, applying vision models to drones in dynamic and stochastic environments. In conjunction with some established knowledge of cryptography and information security, Mohammad is well-suited to apply his skills to the tasks of the image generation pipeline, dataset preparation, and defense performance analysis.

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{bibliography}
}

\end{document}
